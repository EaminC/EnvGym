=== ADJUSTED ENVIRONMENT SETUP PLAN ===

1. DOWNLOADS NEEDED:  
   - Git (latest stable version) for cloning the repository and managing submodules  
   - Conda or Mamba or Micromamba (preferably Micromamba v1.4.1 for best compatibility) for Python environment management  
   - Python 3.11 (matching the environment YAML)  
   - Required Python packages as specified in `BCacheSim/install/env_cachelib-py-3.11.yaml` or `BCacheSim/install/requirements.txt`  
   - Data trace files from https://ftp.pdl.cmu.edu/pub/datasets/Baleen24/ (via `data/get-tectonic.sh` script)  
   - Optional: PyPy environment with Python 3.8 (if faster non-ML runs are desired), installed using `BCacheSim/install/env_cachelib-pypy-3.8.yaml`  
   - Bash shell (Linux native; Windows users should use WSL or Git Bash)  
   - (Optional) Jupyter Notebook or JupyterLab for running and interacting with notebooks  
   - (Optional) SSH client for connecting to dedicated servers on Chameleon cloud  

2. FILES TO CREATE:  
   - `conda` or `mamba` environment from `BCacheSim/install/env_cachelib-py-3.11.yaml` (created by conda command)  
   - (Optional) `conda` environment from `BCacheSim/install/env_cachelib-pypy-3.8.yaml` if PyPy is used  
   - Local clone of repository, including submodules: directory structure as per repo  
   - Downloaded data files placed under `data/` directory after running `data/get-tectonic.sh`  
   - Configuration JSON files for simulation runs (e.g., `runs/example/rejectx/config.json`, `runs/example/baleen/prefetch_ml-on-partial-hit/config.json`) - included in repo or generated by scripts/notebooks  
   - Optional: SSH tunnel configuration files or scripts if connecting to Chameleon dedicated server  
   - (Optional) Jupyter notebook checkpoints and outputs under `notebooks/` for experiment tracking and plotting  

3. NECESSARY TEST CASES IN THE CODEBASE:  
   - Test code to verify simulator baseline run (RejectX policy) executes without error and produces output  
   - Test ML training pipeline for Baleen models completes successfully on sample data  
   - Test simulator run with Baleen ML policy executes without error and generates expected output files  
   - Test data download scripts (`get-tectonic.sh`) correctly download and verify integrity of trace files  
   - Test Jupyter notebooks (e.g., `notebooks/example/example.ipynb`) run end-to-end with no error cells  
   - Test environment activation and dependency resolution (e.g., importing key modules, verifying package versions)  
   - (Optional) Test SSH connection and JupyterHub access on Chameleon cloud environment  

4. COMPLETE TODO LIST:  

   1. **Install system-level prerequisites**  
      - Install Git, Bash shell (Linux native; Windows users should use WSL or Git Bash)  
      - Install Miniconda or Micromamba (recommended Micromamba v1.4.1) for Python environment management  
      - Verify installation:  
        - `git --version`  
        - `conda --version` or `micromamba --version`  

   2. **Clone repository with submodules**  
      - Run:  
        ```
        git clone --recurse-submodules https://github.com/wonglkd/Baleen-FAST24.git
        cd Baleen-FAST24
        ```  
      - Verify that all submodules are cloned: `git submodule status`  

   3. **Create and activate Python environment**  
      - Using Conda/Micromamba on x86_64 Linux platform (recommended platform flag for Docker or container use: `--platform=linux/amd64` if applicable):  
        ```
        conda env create -f BCacheSim/install/env_cachelib-py-3.11.yaml
        conda activate cachelib-py-3.11
        ```  
      - Alternatively, install dependencies with pip (if preferred):  
        ```
        python3 -m pip install --user -r BCacheSim/install/requirements.txt
        ```  
      - Verify Python version is 3.11: `python --version`  
      - Verify key packages installed: `pip list` or `conda list`  
      - Note: No GPU or CUDA dependencies are required or recommended due to lack of NVIDIA GPU detected.  

   4. **Download trace data**  
      - Navigate to data directory (adjust path as per working directory `/home/cc/EnvGym/data-gpt-4.1mini/Baleen` to ensure correct relative or absolute paths):  
        ```
        cd data
        ```  
      - Run data download script:  
        ```
        bash get-tectonic.sh
        ```  
      - Verify presence of trace files in `data/` directory  
      - Optionally, clean and redownload if issues:  
        ```
        bash clean.sh
        bash get-tectonic.sh
        ```  
      - Ensure sufficient storage space (minimum ~10GB free recommended for data traces; current 46.9GB free is sufficient)  

   5. **Run simple baseline simulation (RejectX)**  
      - From repo root, run:  
        ```
        ./BCacheSim/run_py.sh py -B -m BCacheSim.cachesim.simulate_ap --config runs/example/rejectx/config.json
        ```  
      - Verify simulation completes without errors and output files appear in `runs/example/rejectx/`  

   6. **Train Baleen ML models and run Baleen simulation**  
      - Run training command:  
        ```
        ./BCacheSim/run_py.sh py -B -m BCacheSim.episodic_analysis.train --exp example --policy PolicyUtilityServiceTimeSize2 --region Region1 --sample-ratio 0.1 --sample-start 0 --trace-group 201910 --supplied-ea physical --target-wrs 34 50 100 75 20 10 60 90 30 --target-csizes 366.475 --output-base-dir runs/example/baleen --eviction-age 5892.856 --rl-init-kwargs filter_=prefetch --train-target-wr 35.599 --train-models admit prefetch --train-split-secs-start 0 --train-split-secs-end 86400 --ap-acc-cutoff 15 --ap-feat-subset meta+block+chunk
        ```  
      - Run Baleen policy simulation:  
        ```
        ./BCacheSim/run_py.sh py -B -m BCacheSim.cachesim.simulate_ap --config runs/example/baleen/prefetch_ml-on-partial-hit/config.json
        ```  
      - Verify training logs complete without error and simulation outputs are generated in `runs/example/baleen/`  
      - Note: Utilize multi-core CPU capabilities for parallelization if supported by training scripts to maximize performance on x86_64 CPU  

   7. **View results and plots**  
      - Open Jupyter Notebook:  
        ```
        jupyter notebook notebooks/example/example.ipynb
        ```  
      - Run all cells, verify no errors and that plots/data appear as expected  
      - (Optional) Use JupyterLab if preferred  

   8. **(Optional) Use Chameleon Trovi cloud environment**  
      - Request allocation or access to project CHI-231080 via help@chameleoncloud.org  
      - Launch artifact environment via https://www.chameleoncloud.org/experiment/share/aa6fb454-6452-4fc8-994a-b028bfc3c82d  
      - Optionally, open and run notebook `chameleon/1-getting-started.ipynb` to follow interactive setup  
      - For heavier workloads, run `chameleon/2-start-dedicated-server.ipynb` to provision a beefier node  
      - Set up SSH tunnel as instructed to connect to dedicated server  
      - Verify JupyterHub functionality and resource availability  

   9. **Verify overall environment health and reproducibility**  
      - Confirm all scripts run without error  
      - Confirm output files and logs are consistent with expected results  
      - Check for error or warning messages and resolve environment/dependency issues  
      - Ensure `git pull --recurse-submodules` updates repo correctly  
      - Retest data download and cleaning scripts if data errors occur  

   10. **Document and report issues**  
       - If any problems arise, raise GitHub issues at https://github.com/wonglkd/Baleen-FAST24/issues/new  
       - Contact authors or Chameleon helpdesk as needed  

---

**Additional Notes Based on Hardware Information:**

- Architecture: x86_64 Linux confirmed compatible with recommended Docker base images (ubuntu, debian, alpine amd64 tags). Use `--platform=linux/amd64` in Docker or container commands if multi-arch environment to enforce compatibility.

- No NVIDIA GPU detected: Do not install CUDA or GPU-specific dependencies; CPU-only execution is expected.

- Storage: 46.9GB free space is sufficient for data, environments, and outputs; monitor disk usage especially during data downloads and model training outputs.

- Bash shell: Linux native, so all bash scripts (`get-tectonic.sh`, `run_py.sh`, etc.) will run smoothly.

- Docker Version 28.3.2 supports BuildKit and Buildx: leverage these for efficient builds if containerizing, but omit GPU flags.

- Use absolute paths cautiously for volume mounts or script calls to avoid path mismatches; prefer relative paths inside the working directory `/home/cc/EnvGym/data-gpt-4.1mini/Baleen`.

- Memory: Not specified, but ensure enough RAM (8GB+) for Python 3.11 environments and ML training; monitor during heavy workloads.

This adjusted plan ensures full compatibility and smooth setup on your x86_64 Linux environment without GPU support, respecting your storage constraints and path considerations.