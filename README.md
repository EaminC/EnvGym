# 🤖 EnvGym: A Multi-Agent Framework for Reproducing Research Prototypes

<p align="center">
<img align="center" src="assets/title.png" width="498" />
</p>
<p align="center">
      <a href="https://github.com/EaminC/EnvGym/actions" alt="Build status">
    <img src="https://img.shields.io/github/actions/workflow/status/EaminC/EnvGym/build.yml?branch=main" />
  </a>
  <a href="https://github.com/EaminC/EnvGym/releases" alt="Latest Version">
  <img src="https://img.shields.io/github/v/release/EaminC/EnvGym?label=version" />
</a>
    <a href="https://google.github.io/styleguide/javaguide.html" alt="Code style">
        <img src="https://img.shields.io/badge/style-Google-blue" />
    </a>
    <a href="https://dl.acm.org/doi/10.1145/3600006.3613140" alt="SOSP 2023">
        <img src="https://img.shields.io/badge/2025-ICLR-8A2BE2" />
    </a>
    <a href="https://opensource.org/licenses/MIT" alt="License">
        <img src="https://img.shields.io/github/license/EaminC/EnvGym" />
    </a>
</p>

EnvGym is a **general multi-agent framework** designed to **automatically construct executable environments** for reproducing research prototypes from **top-tier academic conferences and journals**.Despite the growing interest in reproducibility, setting up research environments is still **time-consuming and error-prone**. EnvGym tackles this by building a **generalizable agentic system** to automate environment setup—reducing human effort while increasing reliability and repeatability.


Our system leverages the power of **Large Language Model (LLM) agents** to analyze project instructions, resolve dependencies, configure environments, and validate successful execution.

## 🚀 Key Design

<p align="center">
<img align="center" src="assets/Image2.jpg" width="498" />
</p>
<p align="center">
<img align="center" src="assets/Image1.jpg" width="498" />
</p>


## 🚀 Key Features

- ⚙️ **Automatic Environment Construction**  
  Converts research repository instructions into executable environments end-to-end.

- 🤖 **LLM-Powered Agents**  
  Agents use large language models to interpret README files, resolve dependencies, and run setup commands.

- 📐 **Verifiable Rubrics**  
  Fine-grained, programmable evaluation criteria to check correctness and completeness of reproduction.

- ✅ **Auto & Human Evaluation**  
  Automatic LLM-based evaluators, validated by human experts, to ensure execution fidelity.

- 🧩 **Extensible & Modular Design**  
  Built to easily incorporate new agent workflows and debugging tools.

---




## 📂 Repository Structure (WIP)

---

## 📄 Citation (Coming Soon)

We will release a preprint and BibTeX citation for academic use shortly.

---

## 🤝 Contributing

We welcome contributions! Please open issues, submit pull requests, or contact us for collaboration.

---

## 📬 Contact

- **Yiming Cheng** – [GitHub](https://github.com/EaminC) | eaminchan@uchicago.edu
- **Binrui Huang** – [GitHub](https://github.com/samloveshoneywater) | binruih@uchicago.edu

---

## 📜 License

MIT License. See [LICENSE](./LICENSE.md) for details.
